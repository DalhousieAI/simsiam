#!/bin/bash
#SBATCH --time=72:00:00             # max walltime, hh:mm:ss
#SBATCH --nodes 1                   # Number of nodes to request
#SBATCH --gres=gpu:2                # Number of GPUs per node to request
#SBATCH --tasks-per-node=1          # Number of processes to spawn per node
#SBATCH --cpus-per-task=12          # Number of CPUs per GPU
#SBATCH --mem=64G                   # Memory per node
#SBATCH --output=logs/%x_%A-%a_%n-%t.out
                                    # %x=job-name, %A=job ID, %a=array value, %n=node rank, %t=task rank, %N=hostname
                                    # Note: You must manually create output directory "logs" before launching job.
#SBATCH --job-name=simsiam
#SBATCH --account=desired_account   # Use default account

# Manually define this variable to be equal to the number of GPUs in the --gres argument above
GPUS_PER_NODE=2

# Manually define the project name.
# This should also be the name of your conda environment used for this project.
PROJECT_NAME=simsiam

# Exit if any command hits an error
set -e

# sbatch for Graham/Cedar
# Based on https://docs.computecanada.ca/wiki/PyTorch/en
# and https://github.com/facebookresearch/simsiam/blob/main/README.md

# Store the time at which the script was launched
start_time="$SECONDS"

# Print slurm config report
source "slurm/report_slurm_config.sh"
# Print repo status report
source "slurm/report_repo.sh"

echo ""
echo "-------- Input handling ------------------------------------------------"
date
echo ""
# Input handling
DATASET="$1"
shift
echo "DATASET    = $DATASET"

# Take required positional variables out one at a time, using shift to
# pop the argument from the list:

# ARG_NAME="$1" && shift
# echo "ARG_NAME = $ARG_NAME"

# Any remaining arguments will be passed through to the main script later
# (The pass-through works like *args or **kwargs in python.)
echo "EXTRA_ARGS = ${@}"
echo ""

echo "-------- Environment set up --------------------------------------------"
date
echo ""

# Load CC modules
echo ""
echo "Loading modules"
echo ""
module load python/3.8
module load cuda cudnn
module load scipy-stack

# Make an environment, housed on the node's local SSD storage
ENV_DIR="$SLURM_TMPDIR/env"
echo ""
echo "Creating environment $ENV_DIR"
echo ""
virtualenv --no-download "$ENV_DIR"
source "$ENV_DIR/bin/activate"

# Install pytorch
echo ""
echo "Installing packages into $ENV_DIR"
echo ""
# The recommend way is just to do this
python -m pip install --no-index torch==1.9.1 torchvision==0.10.0
python -m pip install -r requirements.txt

# Print env status
source "slurm/report_env_config.sh"

# Set checkpoint and output path environment variables
source "slurm/set_output_paths_cedar.sh"

if [[ "$SLURM_RESTART_COUNT" > 0 && ! -f "$CKPT_DIR/checkpoint_latest.pt" ]];
then
    echo ""
    echo "===================================="
    echo "WARNING:"
    echo "    Resuming after pre-emption (SLURM_RESTART_COUNT=$SLURM_RESTART_COUNT)"
    echo "    but there is no checkpoint file at $CKPT_DIR/checkpoint_latest.pt"
    echo "===================================="
    echo ""
fi;

echo "-------- Dataset transfer ---------------------------------------------"
date
echo ""

if [ "$DATASET" == "imagenet" ]; then
    DATA_DIR="$ROOT_DATA_DIR/$DATASET"
    echo "# Copying imagenet dataset to local node's SSD storage, ${DATA_DIR}"

    rsync -vz /project/rrg-ttt/datasets/imagenet/* "${DATA_DIR}/"

    mkdir -p "$DATA_DIR/train"
    mkdir -p "$DATA_DIR/val"

    echo "Extracting training data"
    tar -C "${DATA_DIR}/train" -xf "${DATA_DIR}/ILSVRC2012_img_train.tar"
    find "${DATA_DIR}/train" -name "*.tar" | while read NAME ; do mkdir -p "${NAME%.tar}"; tar -xf "${NAME}" -C "${NAME%.tar}"; rm -f "${NAME}"; done
    echo ""

    echo "Extracting validation data"
    tar -C "${DATA_DIR}/val" -xf "${DATA_DIR}/ILSVRC2012_img_val.tar"

    # Move validation images to subfolders:
    VAL_PREPPER="$DATA_DIR/valprep.sh"
    if test -f "$VAL_PREPPER"; then
        echo "Downloading valprep.sh";
        wget https://raw.githubusercontent.com/soumith/imagenetloader.torch/b81dbb1/valprep.sh -O "$VAL_PREPPER";
    fi
    echo "Moving validation data into subfolders"
    (cd "${DATA_DIR}/val"; sh "$VAL_PREPPER")
    echo ""

    # Check total files after extract
    #
    #  find train/ -name "*.JPEG" | wc -l
    #  # Should be 1281167
    #  find val/ -name "*.JPEG" | wc -l
    #  # Should be 50000

elif [ "$DATASET" == "imagenette2-160" ] || [ "$DATASET" == "imagenette" ]; then
    DATA_DIR="$ROOT_DATA_DIR/$DATASET"
    echo "# Copying imagenette2-160 dataset to local node's SSD storage, ${DATA_DIR}"

    rsync -zv "/project/rrg-ttt/datasets/imagenette2/imagenette2-160.tgz" "${DATA_DIR}/"

    echo ""
    echo "Extracting data"
    tar -C "${DATA_DIR}" -xf "${DATA_DIR}/imagenette2-160.tgz"
    DATA_DIR="$DATA_DIR/imagenette2-160"

elif [ -d "$DATASET" ]; then
    echo ""
    echo "# Copying directory $DATASET to local node's SSD storage, ${DATA_DIR}"
    rsync -zv "$DATASET" "${DATA_DIR}/"

elif [ -f "$DATASET" ]; then
    echo ""

    DATASET_FNAME="${DATASET##*/}"
    DATASET_NAME="${DATASET_FNAME%%.*}"
    DATASET_EXT="${DATASET_FNAME#*.}"
    TARGET_FNAME="${ROOT_DATA_DIR}/${DATASET_FNAME}"

    if [ "$DATASET_EXT" == "tar.gz" ] || [ "$DATASET_EXT" == "tgz" ] || [ "$DATASET_EXT" == "tar" ]; then
        echo ""
    else
        echo "Unsupported file extension: $DATASET_EXT"
        exit;
    fi
    echo "# Copying file $DATASET to local node's SSD storage, ${DATA_DIR}"
    rsync -zv "$DATASET" "${ROOT_DATA_DIR}/"

    echo "Untarring file $TARGET_FNAME"
    tar -C "${ROOT_DATA_DIR}" -xf "$TARGET_FNAME"
    DATA_DIR="$ROOT_DATA_DIR/$DATASET_NAME"

else
    echo "Invalid dataset name: $DATASET"
    exit;

fi

if [[ "$start_time" != "" ]];
then
    echo "------------------------------------"
    elapsed=$(( SECONDS - start_time ))
    eval "echo Running total elapsed time: $(date -ud "@$elapsed" +'$((%s/3600/24)) days %H hr %M min %S sec')"
fi
echo "------------------------------------------------------------------------"

echo ""
echo "-------- Begin main script ---------------------------------------------"
date
echo ""
#
# Set the number of CPU workers for data processing to match the number of CPUs
# per task (i.e. per node) from the environment variable.
#
N_WORKERS="$SLURM_CPUS_PER_TASK"
echo "N_WORKERS = $N_WORKERS"

export MASTER_ADDR=$(hostname -s)  # Store the master nodeâ€™s IP address in the MASTER_ADDR environment variable.
export MAIN_HOST="$MASTER_ADDR"

echo "r$SLURM_NODEID master: $MASTER_ADDR"
echo "r$SLURM_NODEID Launching python script"

# Get the address of an open socket
source "slurm/get_socket.sh"

# Launch your script
echo ""
echo "# Main script begins with host tcp://localhost:$JOB_SOCKET with backend nccl"
echo ""

# Unsupervised Pre-Training
echo "Unsupervised Pre-Training"
#
# Both the batch size and number of workers get divided by ngpus_per_node in the main_simsiam.py script.
# Batch size parameter is the total batch size across all GPUs on the current node
# We want the batch size to be as large we can, which is about 48 per GPU.
#
BATCH_SIZE="$(( 48 * SLURM_JOB_NUM_NODES * GPUS_PER_NODE ))"
N_WORKERS="$SLURM_JOB_CPUS_PER_NODE"
#
echo "BATCH_SIZE = $BATCH_SIZE"
echo "N_WORKERS  = $N_WORKERS"
#
# We just need to call the main_simsiam script once and it will launch all processes on each node, for each GPU, for us automatically.
#
# To do unsupervised pre-training of a ResNet-50 model on ImageNet, run:
python main_simsiam.py \
    "$DATA_DIR" \
    -a resnet50 \
    --dist-url "tcp://localhost:$JOB_SOCKET" \
    --multiprocessing-distributed \
    --world-size "$SLURM_JOB_NUM_NODES" \
    --rank 0 \
    --fix-pred-lr \
    --workers "$N_WORKERS" \
    --batch-size "$BATCH_SIZE" \
    --checkpoint-dir "$CKPT_DIR" \
    --resume "$CKPT_DIR/checkpoint_latest.pt" \
    "${@}"

echo ""
echo "------------------"
date
elapsed=$(( SECONDS - start_time ))
eval "echo Total elapsed time: $(date -ud "@$elapsed" +'$((%s/3600/24)) days %H hr %M min %S sec')"
echo "------------------------------------"
echo ""

# Linear Classification
echo "Training linear classifier"
# With a pre-trained model, to train a supervised linear classifier on frozen features/weights
python main_lincls.py \
    "$DATA_DIR" \
    -a resnet50 \
    --dist-url "tcp://localhost:$JOB_SOCKET" \
    --multiprocessing-distributed \
    --world-size "$SLURM_JOB_NUM_NODES" \
    --rank 0 \
    --pretrained "$CKPT_DIR/checkpoint_latest.pt" \
    --lars \
    --workers "$N_WORKERS" \
    --batch-size "$BATCH_SIZE" \
    --checkpoint-dir "$CKPT_DIR" \
    --resume "$CKPT_DIR/lincls_checkpoint_latest.pt" \
    "${@}"

echo ""
echo "------------------------------------"
elapsed=$(( SECONDS - start_time ))
eval "echo Running total elapsed time: $(date -ud "@$elapsed" +'$((%s/3600/24)) days %H hr %M min %S sec')"
echo ""
if [[ "$CKPT_DIR" == "" ]];
then
    echo "CKPT_DIR is unset. Will not copy outputs to $JOB_OUTPUT_DIR."
elif [[ "$JOB_OUTPUT_DIR" == "" ]];
then
    echo "JOB_OUTPUT_DIR is unset. Will not copy outputs from $CKPT_DIR."
else
    echo "-------- Saving outputs for long term storage --------------------------"
    date
    echo ""
    echo "Copying outputs from $CKPT_DIR to $JOB_OUTPUT_DIR"
    mkdir -p "$JOB_OUTPUT_DIR"
    rsync -rutlzv "$CKPT_DIR/" "$JOB_OUTPUT_DIR/"
    echo ""
    echo "Output contents of ${JOB_OUTPUT_DIR}:"
    ls -lh "$JOB_OUTPUT_DIR"
fi
echo ""
echo "------------------------------------------------------------------------"
echo ""
echo "Job $SLURM_JOB_NAME ($SLURM_JOB_ID) finished, submitted from $SLURM_SUBMIT_HOST ($SLURM_CLUSTER_NAME)"
date
echo "------------------------------------"
elapsed=$(( SECONDS - start_time ))
eval "echo Total elapsed time: $(date -ud "@$elapsed" +'$((%s/3600/24)) days %H hr %M min %S sec')"
echo "========================================================================"
