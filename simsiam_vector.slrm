#!/bin/bash
#SBATCH --partition=p100,t4v1,t4v2,rtx6000  # Which node partition to use (partitioned by GPU type)
#SBATCH --nodes 1                   # Number of nodes to request
#SBATCH --gres=gpu:4                # Number of GPUs per node to request
#SBATCH --tasks-per-node=1          # Number of processes to spawn per node
#SBATCH --cpus-per-task=24          # Number of CPUs to request
#SBATCH --mem=167G                  # RAM per node (don't exceed 43000MB per GPU)
#SBATCH --output=logs/%x_%A-%a_%n-%t_%N.out
                                    # %x=job-name, %A=job ID, %a=array value, %n=node rank, %t=task rank, %N=hostname
                                    # Note: You must manually create output directory "logs" before launching job.
#SBATCH --job-name=simsiam
#SBATCH --qos=normal
#SBATCH --open-mode=append          # Use append mode otherwise preemption resets the checkpoint file

# sbatch for Vector
# Based on
# https://github.com/VectorInstitute/TechAndEngineering/blob/master/benchmarks/resnet_torch/sample_script/script.sh
# https://github.com/VectorInstitute/TechAndEngineering/blob/master/checkpoint_examples/PyTorch/launch_job.slrm
# https://github.com/VectorInstitute/TechAndEngineering/blob/master/checkpoint_examples/PyTorch/run_train.sh
# https://github.com/facebookresearch/simsiam/blob/main/README.md

date
echo ""
echo "Job $SLURM_JOB_NAME ($SLURM_JOB_ID) begins on $SLURM_NODENAME, submitted from $SLURM_SUBMIT_HOST ($SLURM_CLUSTER_NAME)"
echo ""
echo "Main script will run on host tcp://localhost:10001 with backend nccl"
echo ""
echo "SLURM_CLUSTER_NAME      = $SLURM_CLUSTER_NAME"        # Name of the cluster on which the job is executing.
echo "SLURM_NODENAME          = $SLURM_NODENAME"
echo "SLURM_JOB_QOS           = $SLURM_JOB_QOS"             # Quality Of Service (QOS) of the job allocation.
echo "SLURM_JOB_ID            = $SLURM_JOB_ID"              # The ID of the job allocation.
echo "SLURM_RESTART_COUNT     = $SLURM_RESTART_COUNT"       # The number of times the job has been restarted.
if [ "$SLURM_ARRAY_TASK_COUNT" != "" ] && [ "$SLURM_ARRAY_TASK_COUNT" -gt 1 ]; then
    echo ""
    echo "SLURM_ARRAY_JOB_ID      = $SLURM_ARRAY_JOB_ID"        # Job array's master job ID number.
    echo "SLURM_ARRAY_TASK_COUNT  = $SLURM_ARRAY_TASK_COUNT"    # Total number of tasks in a job array.
    echo "SLURM_ARRAY_TASK_ID     = $SLURM_ARRAY_TASK_ID"       # Job array ID (index) number.
    echo "SLURM_ARRAY_TASK_MAX    = $SLURM_ARRAY_TASK_MAX"      # Job array's maximum ID (index) number.
    echo "SLURM_ARRAY_TASK_STEP   = $SLURM_ARRAY_TASK_STEP"     # Job array's index step size.
fi;
echo ""
echo "SLURM_JOB_NUM_NODES     = $SLURM_JOB_NUM_NODES"       # Total number of nodes in the job's resource allocation.
echo "SLURM_JOB_NODELIST      = $SLURM_JOB_NODELIST"        # List of nodes allocated to the job.
echo "SLURM_TASKS_PER_NODE    = $SLURM_TASKS_PER_NODE"      # Number of tasks to be initiated on each node.
echo "SLURM_NTASKS            = $SLURM_NTASKS"              # Number of tasks to spawn.
echo "SLURM_PROCID            = $SLURM_PROCID"              # The MPI rank (or relative process ID) of the current process
echo ""
echo "SBATCH_GRES             = $SBATCH_GRES"               # Same as --gres
echo "SLURM_GPUS              = $SLURM_GPUS"                # Number of GPUs requested. Only set if the -G, --gpus option is specified.
echo "SLURM_GPUS_PER_NODE     = $SLURM_GPUS_PER_NODE"       # Requested GPU count per allocated node. Only set if the --gpus-per-node option is specified.
echo "SLURM_GPUS_PER_TASK     = $SLURM_GPUS_PER_TASK"       # Requested GPU count per allocated task. Only set if the --gpus-per-task option is specified.
echo "SLURM_CPUS_ON_NODE      = $SLURM_CPUS_ON_NODE"        # Number of CPUs allocated to the batch step.
echo "SLURM_JOB_CPUS_PER_NODE = $SLURM_JOB_CPUS_PER_NODE"   # Count of CPUs available to the job on the nodes in the allocation.
echo "SLURM_CPUS_PER_TASK     = $SLURM_CPUS_PER_TASK"       # Number of cpus requested per task. Only set if the --cpus-per-task option is specified.
echo ""
echo "------------------------------------------------------------------------"
echo ""
# Input handling
DATASET="$1"
echo "DATASET = $DATASET"
echo ""
echo "------------------------------------------------------------------------"
echo ""
pwd
echo ""
echo "commit ref:"
git rev-parse HEAD
echo ""
git status
echo ""
echo "------------------------------------------------------------------------"
echo ""
date
echo ""
echo "# Activating the environment"

source ~/.bashrc
ENVNAME=simsiam
conda activate "$HOME/venvs/$ENVNAME"

echo "------------------------------------------------------------------------"
echo ""
date
echo ""
echo "# Debugging outputs"
echo ""
echo "pwd:"
pwd
echo ""
echo "python --version"
python --version
echo ""
echo "which conda:"
which conda
echo ""
echo "conda info:"
conda info
echo ""
echo "conda export:"
conda env export
echo ""
echo "which pip:"
which pip
echo ""
echo "pip freeze:"
echo ""
pip freeze
echo ""
echo "which nvcc:"
which nvcc
echo ""
echo "nvcc --version"
nvcc --version
echo ""
echo "nvidia-smi"
nvidia-smi
echo ""
python -c "import torch; print('pytorch={}, cuda={}, gpus={}'.format(torch.__version__, torch.cuda.is_available(), torch.cuda.device_count()))"
echo ""
python -c "import torch; print(str(torch.zeros(2, device=torch.device('cuda')))); print('able to use cuda')"
echo ""
echo "------------------------------------------------------------------------"
echo "# Handling data and checkpoint paths"

# Vector provides a fast parallel filesystem local to the GPU nodes,  dedicated
# for checkpointing. It is mounted under /checkpoint. It is strongly
# recommended that you keep your intermediary checkpoints under this directory
CKPT_DIR="/checkpoint/${USER}/${SLURM_JOB_ID}"

# We also recommend users to create a symlink of the checkpoint dir so your
# training code stays the same with regards to different job IDs and it would
# be easier to navigate the checkpoint directory under your job's working directory
ln -sfn "$CKPT_DIR" "$PWD/checkpoint"

# In the future, the checkpoint directory will be removed immediately after the
# job has finished. If you would like the file to stay longer, and create an
# empty "delay purge" file as a flag so the system will delay the removal for
# 48 hours
touch "$CKPT_DIR/DELAYPURGE"

echo ""
echo "df -h:"
df -h
echo ""

echo "ls -lh ${SLURM_TMPDIR}:"
ls -lh "${SLURM_TMPDIR}"
echo ""

echo "ls -lh ${CKPT_DIR}:"
ls -lh "${CKPT_DIR}"
echo ""

# -------------------------------------------------------------------------
echo ""
date
echo ""

if [ "$DATASET" == "imagenet" ]; then
    DATA_DIR="/scratch/ssd002/datasets/imagenet"

elif [ "$DATASET" == "imagenette2-160" ]; then
    DATA_DIR="/scratch/ssd004/datasets/imagenette2/160"

else
    echo "Invalid dataset name: $DATASET"
    exit;

fi

echo ""
echo "------------------------------------------------------------------------"
echo ""
date
echo ""

export MASTER_ADDR=$(hostname -s)  # Store the master nodeâ€™s IP address in the MASTER_ADDR environment variable.
export MAIN_HOST="$MASTER_ADDR"

echo "r$SLURM_NODEID master: $MASTER_ADDR"
echo "r$SLURM_NODEID Launching python script"

# Launch your script
echo ""
echo "# Main script begins with host tcp://localhost:10001 with backend nccl"
echo ""

# Unsupervised Pre-Training
echo "Unsupervised Pre-Training"
#
# Both the batch size and number of workers get divided by ngpus_per_node in the main_simsiam.py script.
# Batch size parameter is the total batch size across all GPUs on the current node
# We want the batch size to be as large we can, which is about 48 per GPU.
#
BATCH_SIZE="$(( 48 * SLURM_JOB_NUM_NODES * SLURM_TASKS_PER_NODE ))"
N_WORKERS="$(( SLURM_CPUS_PER_TASK * SLURM_TASKS_PER_NODE ))"
#
#
# We just need to call the main_simsiam script once and it will launch all processes on each node, for each GPU, for us automatically.
#
# To do unsupervised pre-training of a ResNet-50 model on ImageNet, run:
python main_simsiam.py \
    -a resnet50 \
    --dist-url 'tcp://localhost:10001' \
    --multiprocessing-distributed \
    --world-size "$SLURM_JOB_NUM_NODES" \
    --rank 0 \
    --fix-pred-lr \
    --workers "$N_WORKERS" \
    --batch-size "$BATCH_SIZE" \
    --checkpoint-dir "$CKPT_DIR" \
    "$DATA_DIR"

echo ""
echo "------------------------------------------------------------------------"
echo ""
date
echo ""

# Linear Classification
echo "Training linear classifier"
# With a pre-trained model, to train a supervised linear classifier on frozen features/weights
python main_lincls.py \
    -a resnet50 \
    --dist-url 'tcp://localhost:10001' \
    --multiprocessing-distributed \
    --world-size "$SLURM_JOB_NUM_NODES" \
    --rank 0 \
    --pretrained "$CKPT_DIR/checkpoint_latest.pt" \
    --lars \
    --workers "$N_WORKERS" \
    --batch-size "$BATCH_SIZE" \
    --checkpoint-dir "$CKPT_DIR" \
    "$DATA_DIR"
