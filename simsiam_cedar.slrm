#!/bin/bash
#SBATCH --time=72:00:00             # max walltime, hh:mm:ss
#SBATCH --nodes 1                   # Number of nodes to request.
#SBATCH --gres=gpu:4                # Number of GPUs per node to request.
#SBATCH --tasks-per-node=4          # Set this to equal the number of GPUs per node.
#SBATCH --cpus-per-task=6           # Number of CPUs per GPU-worker.
#SBATCH --mem=125G                  # Memory per node
#SBATCH --output=logs/%A-%a_%N.out  # %N for node name, %j for jobID. **You must manually create output directory "logs" before launching job.**
#SBATCH --job-name=simsiam

# sbatch for Graham/Cedar
# Based on https://docs.computecanada.ca/wiki/PyTorch/en
# and https://github.com/facebookresearch/simsiam/blob/main/README.md

date
echo ""
echo "Job $SLURM_JOB_NAME ($SLURM_JOB_ID) begins on $SLURM_SUBMIT_HOST"
echo ""
echo "Main script will run on host tcp://localhost:10001 with backend nccl"
echo ""
echo "SLURM_JOB_NUM_NODES  = $SLURM_JOB_NUM_NODES"
echo "SLURM_JOB_NODELIST   = $SLURM_JOB_NODELIST"
echo "SLURM_TASKS_PER_NODE = $SLURM_TASKS_PER_NODE"
echo "SLURM_JOB_ID         = $SLURM_JOB_ID"
echo "SLURM_PROCID         = $SLURM_PROCID"
echo "SLURM_NTASKS         = $SLURM_NTASKS"
echo "SLURM_CPUS_ON_NODE   = $SLURM_CPUS_ON_NODE"
echo "SLURM_CPUS_PER_TASK  = $SLURM_CPUS_PER_TASK"

echo "------------------------------------------------------------------------"
echo ""
date
echo ""
echo "# Starting environment set up"

# Load CC modules
echo ""
echo "Loading modules"
echo ""
module load python/3.8
module load cuda cudnn
module load scipy-stack

# Make an environment, housed on the node's local SSD storage
ENV_DIR="$SLURM_TMPDIR/env"
echo ""
echo "Creating environment $ENV_DIR"
echo ""
virtualenv --no-download "$ENV_DIR"
source "$ENV_DIR/bin/activate"

# Install pytorch
echo ""
echo "Installing packages into $ENV_DIR"
echo ""
python -m pip install --no-index torch torchvision
python -m pip install -r requirements.txt
python -m pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" git+https://github.com/NVIDIA/apex

echo "------------------------------------------------------------------------"
echo ""
date
echo ""
echo "# Debugging outputs"
echo ""
echo "pwd:"
pwd
echo ""
echo "python --version"
python --version
echo ""
echo "which pip:"
which pip
echo ""
echo "pip freeze:"
echo ""
pip freeze
echo ""
echo "which nvcc:"
which nvcc
echo ""
echo "nvcc --version"
nvcc --version
echo ""
echo "nvidia-smi"
nvidia-smi
echo ""
python -c "import torch; print('pytorch={}, cuda={}, gpus={}'.format(torch.__version__, torch.cuda.is_available(), torch.cuda.device_count()))"
echo ""
python -c "import torch; print(str(torch.zeros(2, device=torch.device('cuda')))); print('able to use cuda')"
echo ""
echo "------------------------------------------------------------------------"
echo "# Handling data on the node"
echo ""
echo "df -h:"
df -h
echo ""

echo "ls -lh ${SLURM_TMPDIR}:"
ls -lh "${SLURM_TMPDIR}"
echo ""

DATA_DIR="${SLURM_TMPDIR}/imagenet"
mkdir -p "$DATA_DIR"

echo "ls -lh ${DATA_DIR}:"
ls -lh "${DATA_DIR}"
echo ""

CKPT_DIR="${SLURM_TMPDIR}/checkpoint"
mkdir -p "$CKPT_DIR"

echo "ls -lh ${CKPT_DIR}:"
ls -lh "${CKPT_DIR}"
echo ""

# -------------------------------------------------------------------------
echo ""
date
echo ""
echo "# Copying imagenet dataset to local node's SSD storage, ${DATA_DIR}"

rsync -vz /project/rrg-ttt/datasets/imagenet/* "${DATA_DIR}/"

mkdir -p "$DATA_DIR/train"
mkdir -p "$DATA_DIR/val"

echo "Extracting training data"
tar -C "${DATA_DIR}/train" -xf "${DATA_DIR}/ILSVRC2012_img_train.tar"
find "${DATA_DIR}/train" -name "*.tar" | while read NAME ; do mkdir -p "${NAME%.tar}"; tar -xf "${NAME}" -C "${NAME%.tar}"; rm -f "${NAME}"; done
echo ""

echo "Extracting validation data"
tar -C "${DATA_DIR}/val" -xf "${DATA_DIR}/ILSVRC2012_img_val.tar"

# Move validation images to subfolders:
VAL_PREPPER="$DATA_DIR/valprep.sh"
if test -f "$VAL_PREPPER"; then
    echo "Downloading valprep.sh";
    wget https://raw.githubusercontent.com/soumith/imagenetloader.torch/b81dbb1/valprep.sh -O "$VAL_PREPPER";
fi
echo "Moving validation data into subfolders"
(cd "${DATA_DIR}/val"; sh "$VAL_PREPPER")
echo ""

# Check total files after extract
#
#  find train/ -name "*.JPEG" | wc -l
#  # Should be 1281167
#  find val/ -name "*.JPEG" | wc -l
#  # Should be 50000

echo "------------------------------------------------------------------------"
echo ""
date
echo ""

export NCCL_BLOCKING_WAIT=1     # Set this if you want to use the NCCL backend for inter-GPU communication.
export MASTER_ADDR=$(hostname)  # Store the master nodeâ€™s IP address in the MASTER_ADDR environment variable.

echo "r$SLURM_NODEID master: $MASTER_ADDR"
echo "r$SLURM_NODEID Launching python script"

# Launch your script
echo ""
echo "# Main script begins with host tcp://localhost:10001 with backend nccl"
echo ""

# Unsupervised Pre-Training
echo "Unsupervised Pre-Training"
#
# Both the batch size and number of workers get divided by ngpus_per_node in the main_simsiam.py script.
# Batch size parameter is the total batch size across all GPUs on the current node
# We want the actual batch size to be 64 per GPU, so the value to use is 64 * number of GPUs per node
#
BATCH_SIZE="$(( 64 * SLURM_JOB_NUM_NODES * SLURM_TASKS_PER_NODE ))"
N_WORKERS="$(( SLURM_CPUS_PER_TASK * SLURM_TASKS_PER_NODE ))"
#
#
# We just need to call the main_simsiam script once and it will launch all processes on each node, for each GPU, for us automatically.
#
# To do unsupervised pre-training of a ResNet-50 model on ImageNet, run:
python main_simsiam.py \
    -a resnet50 \
    --dist-url 'tcp://localhost:10001' \
    --multiprocessing-distributed \
    --world-size "$SLURM_JOB_NUM_NODES" \
    --rank 0 \
    --fix-pred-lr \
    --workers "$N_WORKERS" \
    --batch-size "$BATCH_SIZE" \
    --checkpoint-dir "$CKPT_DIR" \
    "$DATA_DIR"

echo ""
echo "------------------------------------------------------------------------"
echo ""
date
echo ""

# Linear Classification
echo "Training linear classifier"
# With a pre-trained model, to train a supervised linear classifier on frozen features/weights
python main_lincls.py \
    -a resnet50 \
    --dist-url 'tcp://localhost:10001' \
    --multiprocessing-distributed \
    --world-size "$SLURM_JOB_NUM_NODES" \
    --rank 0 \
    --pretrained "$CKPT_DIR/checkpoint_latest.pt" \
    --lars \
    --workers "$N_WORKERS" \
    --batch-size "$BATCH_SIZE" \
    --checkpoint-dir "$CKPT_DIR" \
    "$DATA_DIR"
